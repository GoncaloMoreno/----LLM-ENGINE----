{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1108aead",
   "metadata": {},
   "source": [
    "My initial idea, creating my own stuff semi based on pytorch before totaly torch'ing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d290d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from natsort import natsorted\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "root = Path().resolve().parent\n",
    "\n",
    "folder = root / '1 DATA + CLEANUP' / 'game_files' / 'train tokenized'\n",
    "idx_folder = root / '1 DATA + CLEANUP' / 'game_files' / 'train maps'\n",
    "\n",
    "# dont know if to use normal Dataset or IterableDataset from pytorch\n",
    "\n",
    "class my_train_Dataset(): # supposed to be used with train files and maps\n",
    "\n",
    "    def __init__(self, in_folder = folder, idx_folder = idx_folder):\n",
    "\n",
    "        self.in_folder = in_folder  # folder with chunk files\n",
    "        self.idx_folder = idx_folder    # folder with map files\n",
    "        self.gpchunk = []    # number of games per file / map\n",
    "\n",
    "        # chunk and map variables\n",
    "        self.current_chunk = None\n",
    "        self.current_map = None\n",
    "        self.n_chunks = None\n",
    "        self.current_chunk_data = None\n",
    "        self.current_map_data = None\n",
    "\n",
    "        # functions to run on init\n",
    "        self.game_count()\n",
    "        self.cm_tuple()\n",
    "\n",
    "    def game_count(self): # number of games in each chunk\n",
    "        if 'train' in str(self.in_folder):\n",
    "            for i in range(len(self.in_chunks)): # pre-computed\n",
    "                n = 2_500_000\n",
    "                if i == 9: n = 2456165\n",
    "                if i == 19: n = 2451750\n",
    "                if i == 29: n = 2450871\n",
    "                self.gpchunk.append([i, n])\n",
    "\n",
    "    def cm_tuple(self):   # chunk-maps tuples\n",
    "        self.in_chunks = natsorted([f for f in self.in_folder.iterdir()])   # list of chunk paths\n",
    "        self.n_chunks = len(self.in_chunks)\n",
    "        self.maps = natsorted([f for f in self.idx_folder.iterdir()])   # list of map paths\n",
    "        if len(self.in_chunks) != len(self.maps):   # shouldnt need to be ran but just in case\n",
    "            raise ValueError(\"Number of input chunks(files) and index files do not match.\")\n",
    "        else:\n",
    "            self.chunks_maps = {i:t for i, t in enumerate(zip(self.in_chunks, self.maps))} # (chunk, map)\n",
    "\n",
    "    def get_chunk(self, chunk_idx): # helper function to load chunk and map data\n",
    "        if chunk_idx >= len(self.chunks_maps) or chunk_idx < 0:\n",
    "            raise IndexError(\"Index out of range.\")\n",
    "        chunk, map = self.chunks_maps[chunk_idx]\n",
    "        if chunk == self.current_chunk and map == self.current_map: # dont reload if same chunk\n",
    "            return self.current_chunk_data, self.current_map_data\n",
    "        self.current_chunk = chunk  # chunk file path\n",
    "        self.current_map = map  # map file path\n",
    "        self.current_chunk_data = torch.load(chunk)   # chunk data (flat tensor of ~2.5M tokenized games)\n",
    "        with open(map) as m: self.current_map_data = json.load(m)  # map data (list of tuples with game indices)\n",
    "        return self.current_chunk_data, self.current_map_data\n",
    "\n",
    "    def __len__(self):  # total number of games in all chunks (idk, pytorch says it wants it)\n",
    "        return 2_500_000 * 33 + 2456165 + 2451750 + 2450871\n",
    "    \n",
    "    def __getitem__(self, chunk_idx, game_idx): # get game from chunk\n",
    "        self.get_chunk(chunk_idx)\n",
    "        return self.current_chunk_data[self.current_map_data[game_idx][0]:self.current_map_data[game_idx][1]] # game_idx'th game from chunk_idx'th chunk\n",
    "\n",
    "#############################\n",
    "\n",
    "class my_Sampler(): # sampler for train data\n",
    "\n",
    "    def __init__(self, dataset, shuffle_chunks=True, shuffle_games=True):\n",
    "        self.chunk_order = torch.arange(len(dataset.self.n_chunks))\n",
    "        self.game_order = torch.arange(2_500_000) # 2.5M games per chunk (tho some arent... idk yet)\n",
    "        self.shuffle_chunks = shuffle_chunks\n",
    "        self.shuffle_games = shuffle_games\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        if self.shuffle_chunks:\n",
    "            self.chunks_order = torch.randperm(len(dataset.self.n_chunks))\n",
    "        if self.shuffle_games: # idk how to make the order dependent on the chunk size, some are smaller\n",
    "            self.games_order = torch.randperm(2_500_000).to_list()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for chunk_idx in self.chunk_order:\n",
    "            for game_idx in self.game_order:\n",
    "                yield chunk_idx, game_idx\n",
    "\n",
    "##########################\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    games = [tensor for tensor in batch]  # list of game tensors\n",
    "    games = pad_sequence(games, batch_first=True, padding_value=0)  # pad to max length in batch\n",
    "    return games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e0557",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d82ca4",
   "metadata": {},
   "source": [
    "change after help from AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bd638c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from natsort import natsorted\n",
    "import random\n",
    "import json\n",
    "#import dataloader\n",
    "\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from pathlib import Path\n",
    "root = Path().resolve().parent\n",
    "\n",
    "c_folder = root / '1 DATA + CLEANUP' / 'game_files' / 'train tokenized'\n",
    "map_folder = root / '1 DATA + CLEANUP' / 'game_files' / 'train maps'\n",
    "\n",
    "class my_Dataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, in_folder, idx_folder, shuffle=True):\n",
    "        self.shuffle = shuffle\n",
    "        # chunk and map paths\n",
    "        self.in_folder = in_folder\n",
    "        self.idx_folder = idx_folder\n",
    "        # chunk and map variables\n",
    "        self.chunks_maps = None\n",
    "        # functions to run on init\n",
    "        self.cm_tuple()\n",
    "\n",
    "    def cm_tuple(self):   # chunk-maps tuples\n",
    "        in_chunks = natsorted([f for f in self.in_folder.iterdir() if '.pt' in str(f)])   # list of chunk paths (without game_counts)\n",
    "        self.n_chunks = len(in_chunks)\n",
    "        maps = natsorted([f for f in self.idx_folder.iterdir()])   # list of map paths\n",
    "        if len(in_chunks) != len(maps):   # shouldnt need to be ran but just in case\n",
    "            raise ValueError(\"Number of input chunks(files) and index files do not match.\")\n",
    "        else:\n",
    "            self.chunks_maps = list(zip(in_chunks, maps)) # (chunk, map)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # --- Worker information and workload splitting ---\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "\n",
    "        # --- Determine the chunks for this worker ---\n",
    "        all_chunk_map_pairs = list(self.chunks_maps) # Get the original list (make copy if needed)\n",
    "        num_chunks = len(all_chunk_map_pairs)\n",
    "\n",
    "        if num_chunks == 0:\n",
    "            return # Stop iteration if dataset is empty\n",
    "\n",
    "        # Calculate the range of chunks this worker should process (based on original order)\n",
    "        per_worker = int(math.ceil(num_chunks / float(num_workers)))\n",
    "        iter_start = worker_id * per_worker\n",
    "        iter_end = min(iter_start + per_worker, num_chunks)\n",
    "\n",
    "        # Get the subset of chunk paths assigned to this worker\n",
    "        assigned_chunk_map_pairs = all_chunk_map_pairs[iter_start:iter_end] # Slice FIRST\n",
    "\n",
    "        # Shuffle the subset assigned to this worker (optional)\n",
    "        if self.shuffle:\n",
    "            random.shuffle(assigned_chunk_map_pairs) # Shuffle the worker's ASSIGNED portion SECOND\n",
    "\n",
    "        # --- Process assigned chunks ---\n",
    "        print(f\"[Worker {worker_id}] Processing {len(assigned_chunk_map_pairs)} chunks (indices from original list: {iter_start} to {iter_end-1}).\") # Optional debug print\n",
    "\n",
    "        for chunk_path, map_path in assigned_chunk_map_pairs:\n",
    "            try:\n",
    "                # Load data for the current chunk\n",
    "                # Consider adding more robust file existence checks if needed\n",
    "                if not chunk_path.exists() or not map_path.exists():\n",
    "                     print(f\"[Worker {worker_id}] Warning: File missing for chunk {chunk_path.name} or map {map_path.name}. Skipping.\")\n",
    "                     continue\n",
    "                     \n",
    "                # print(f\"[Worker {worker_id}] Loading chunk: {chunk_path.name}\") # Optional debug print\n",
    "                chunk_data = torch.load(chunk_path)\n",
    "                with open(map_path, 'r') as f:\n",
    "                    map_data = json.load(f) # Should be a list of [start, end] pairs\n",
    "\n",
    "                num_games_in_chunk = len(map_data)\n",
    "                if num_games_in_chunk == 0:\n",
    "                    # print(f\"[Worker {worker_id}] Warning: Chunk {chunk_path.name} has 0 games in map. Skipping.\") # Optional debug print\n",
    "                    continue # Skip empty chunks\n",
    "\n",
    "                # Generate local game indices (0 to num_games-1)\n",
    "                local_game_indices = list(range(num_games_in_chunk))\n",
    "\n",
    "                # Shuffle local game indices within the chunk (optional)\n",
    "                if self.shuffle:\n",
    "                    random.shuffle(local_game_indices)\n",
    "\n",
    "                # Yield games from this chunk based on shuffled local indices\n",
    "                for game_idx in local_game_indices:\n",
    "                    try:\n",
    "                        start, end = map_data[game_idx]\n",
    "                        yield chunk_data[start:end]\n",
    "                    except IndexError:\n",
    "                         print(f\"[Worker {worker_id}] Error: Game index {game_idx} out of bounds for map {map_path.name} (len={num_games_in_chunk}). Skipping game.\")\n",
    "                    except Exception as game_e:\n",
    "                         print(f\"[Worker {worker_id}] Error yielding game {game_idx} from chunk {chunk_path.name}: {game_e}\")\n",
    "                         \n",
    "                # Optional: Explicitly free memory for the large chunk data\n",
    "                del chunk_data\n",
    "                del map_data\n",
    "                del local_game_indices\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                 print(f\"[Worker {worker_id}] Error: File not found for chunk {chunk_path} or map {map_path}. Skipping chunk.\")\n",
    "            except Exception as e:\n",
    "                # Log error or decide how to handle other corrupted files/chunks\n",
    "                print(f\"[Worker {worker_id}] Error processing chunk {chunk_path.name}: {e}\")\n",
    "                # Optionally 'del chunk_data' etc. here too in case they exist\n",
    "                continue # Skip to the next chunk\n",
    "\n",
    "##########################\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    games = [tensor for tensor in batch]  # list of game tensors\n",
    "    games = pad_sequence(games, batch_first=True, padding_value=0)  # pad to max length in batch\n",
    "    return games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fb8931eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = my_Dataset(c_folder, map_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ad4a1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- How to use it ---\n",
    "# dataset = ChessIterableDataset(chunk_folder='/path/to/chunks', map_folder='/path/to/maps')\n",
    "# # IMPORTANT: collate_fn is still needed for padding batches!\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=32,\n",
    "#     num_workers=4, # Example: Use 4 worker processes\n",
    "#     collate_fn=my_collate_fn, # Your function to pad sequences in a batch\n",
    "#     pin_memory=True # Often good practice if using GPU\n",
    "# )\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "#     for i, batch in enumerate(dataloader):\n",
    "#         # Your training step here with the batch\n",
    "#         print(f\"Processing batch {i}, shape: {batch.shape}\") # Example\n",
    "#         # model(batch) ... etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b5ae908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, num_workers=2, collate_fn=my_collate_fn)#, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs but need gpu\n",
    "for epoch in range(1):\n",
    "    print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Your training step here with the batch\n",
    "        print(f\"Processing batch {i}, shape: {batch.shape}\") # Example\n",
    "        # model(batch) ... etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
