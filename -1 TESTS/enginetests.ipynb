{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import struct\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tokenbasic import BasicTokenizer\n",
    "from chessdataset import BinaryPGNDataset, collate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\moren\\Desktop\\Masters\\2nd Semester\\Deep Learning\\personal engine\\game_files\\tcec_games_cleaned.txt') as f:\n",
    "    games = f.read()\n",
    "\n",
    "list_games = games.split('\\n')\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(r'C:\\Users\\moren\\Desktop\\Masters\\2nd Semester\\Deep Learning\\personal engine\\token_files\\minbpeTokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_dataset(file_path, tokenizer, outfile_path):\n",
    "    # Store input_ids and their lengths\n",
    "    tokenized_data = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            pgn_text = line.strip()\n",
    "            if not pgn_text:\n",
    "                continue\n",
    "                \n",
    "            # Tokenize\n",
    "            input_ids = tokenizer.encode(pgn_text)\n",
    "            \n",
    "            # Store the input_ids and their length\n",
    "            tokenized_data.append((input_ids, len(input_ids)))\n",
    "    \n",
    "    # Sort by length (second element of each tuple)\n",
    "    tokenized_data.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Extract just the input_ids after sorting\n",
    "    sorted_input_ids = [item[0] for item in tokenized_data]\n",
    "    \n",
    "    # Save the sorted dataset\n",
    "    torch.save(sorted_input_ids, outfile_path)\n",
    "    \n",
    "    return sorted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tokenized_dataset('games5000.txt', tokenizer, 'games5000token.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.load('games5000token.pt')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BinaryPGNDataset(r'C:\\Users\\moren\\Desktop\\Masters\\2nd Semester\\Deep Learning\\personal engine\\game_files\\tcec_games_tokenized.bin')\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# remember masking of padding token when on transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    firstbatch = batch\n",
    "    break\n",
    "# longest batch = 1944, smallest batch = 330, average = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[295, 285, 320, 354, 506, 336, 319, 284, 310, 370, 293, 421, 371, 275,\n",
       "          78, 296, 363, 102, 266, 291, 329, 290, 278, 395, 305,  66, 350, 337,\n",
       "         314, 313,  49, 338, 311, 426, 294, 295, 340, 421,  49, 315, 276,  81,\n",
       "         420, 383, 288, 103, 292, 352,  99, 343,  99, 435, 371, 282, 364, 102,\n",
       "         266,  49, 403, 120, 102, 266, 478,  49, 329, 478, 369, 492,  49, 368,\n",
       "         120,  99, 330, 469,  57, 438,  78, 494, 373, 309, 364, 288,  50, 396,\n",
       "         120, 288,  78, 103, 334,  50, 339, 492, 323,  50, 380, 276, 303,  50,\n",
       "         360, 305,  78, 299,  50, 375, 322,  81, 289,  50, 385, 273, 261,  81,\n",
       "         273, 263,  50, 349, 276,  81, 300,  50, 368, 272,  81, 494, 346, 308,\n",
       "         414, 499, 338, 468, 272,  78, 300,  51,  49, 328, 320, 418, 499,  50,\n",
       "         317, 259, 425, 312,  51, 328, 303, 369, 379,  51,  52, 317, 431, 302,\n",
       "          51,  53, 317, 261,  78, 285,  51,  54, 328, 102, 407, 103, 312, 349,\n",
       "         282, 418, 288,  51, 335, 443, 288, 439,  51, 346, 272,  82, 305,  52,\n",
       "         408, 275,  82, 285,  52, 347, 282, 419, 266,  52,  50, 345, 281, 498,\n",
       "          52, 348, 272, 419, 266,  52, 324, 275,  82, 286,  52, 406, 465, 120,\n",
       "         340,  52, 344, 449,  82, 293,  52, 349, 307,  82, 342,  52, 412, 286,\n",
       "          82, 306,  52, 346, 331, 470,  53, 338, 282, 377, 394,  53, 347, 293,\n",
       "          82, 306,  53, 339, 307, 377, 394,  53, 411, 357, 439,  53,  52, 345,\n",
       "         281, 308,  53, 406, 327,  82, 296,  53, 344, 100, 407, 103, 362, 349,\n",
       "         299,  82, 306,  53, 355, 101, 431, 302,  53, 346, 445, 296,  54, 408,\n",
       "         314, 425, 266,  54, 347, 104, 281, 296,  54,  50, 317, 281, 280,  54,\n",
       "         411, 327,  82, 296,  54, 352, 489, 306,  54, 351,  99, 407, 103, 413,\n",
       "         344, 299,  82, 296,  54, 349, 298,  82, 306,  54, 355, 307, 439,  54,\n",
       "         346, 320,  82, 280,  55, 353,  98, 407, 103, 436, 347, 289, 439,  55,\n",
       "         339, 308,  82, 306,  55, 348, 276,  82, 296,  55, 352, 481, 306,  55,\n",
       "         351, 101, 356, 318,  55, 344, 286, 439,  55, 349, 290,  82, 296,  55,\n",
       "         355, 476, 306,  55, 346, 104,  49]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output proj\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # bias / mask (openai/HF naming)\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x, padding_mask= None):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dim\n",
    "        # calculate query, key, values for all heads\n",
    "        # nh = n of heads, hs = head size, C = n channels = nh * hs\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, 1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble head outputs side by side\n",
    "        # output proj\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "               \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp  = MLP(config)\n",
    "\n",
    "    def forward(self, x):                \n",
    "        x = x + self.attn(self.ln_1(x))     # x -> layer norm -> attention\n",
    "        x = x + self.mlp(self.ln_2(x))      #   -> layer norm -> mlp\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class CLLMConfig:\n",
    "    block_size: int = 1024      # max context length\n",
    "    vocab_size: int = 513       # n unique tokens\n",
    "    n_layer:    int = 4         # n of transformer layers\n",
    "    n_head:     int = 4         # n of heads\n",
    "    n_embd:     int = 128       # embedding dimension\n",
    "\n",
    "class CLLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "    \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte  = nn.Embedding(config.vocab_size, config.n_embd),                 # token embeddings\n",
    "            wpe  = nn.Embedding(config.block_size, config.n_embd),                 # position embeddings\n",
    "            h    = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # number of hidden layers\n",
    "            ln_f = nn.LayerNorm(config.n_embd),                                    # final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)                 # \n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f'Cannot forward sequence of length {T}'\n",
    "        # forward the tok and pos embeddings\n",
    "        pos = torch.arange(0, T, dtype= torch.long, device= idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos) # pos embds, shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # tok embs, shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAUDE'S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output proj\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # bias / mask (openai/HF naming)\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # Apply causal mask\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        \n",
    "        # Additionally mask padding tokens if padding_mask is provided\n",
    "        if padding_mask is not None:\n",
    "            # Convert padding_mask from [B, T] to [B, 1, 1, T] for broadcasting\n",
    "            pad_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            att = att.masked_fill(~pad_mask.bool(), float('-inf'))\n",
    "        \n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "               \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        # Pass padding_mask to attention\n",
    "        x = x + self.attn(self.ln_1(x), padding_mask=padding_mask)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class CLLMConfig:\n",
    "    block_size: int = 1024      # max context length\n",
    "    vocab_size: int = 513       # n unique tokens\n",
    "    n_layer:    int = 4         # n of transformer layers\n",
    "    n_head:     int = 4         # n of heads\n",
    "    n_embd:     int = 128       # embedding dimension\n",
    "\n",
    "class CLLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "    \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte  = nn.Embedding(config.vocab_size, config.n_embd),                 # token embeddings\n",
    "            wpe  = nn.Embedding(config.block_size, config.n_embd),                 # position embeddings\n",
    "            h    = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # number of hidden layers\n",
    "            ln_f = nn.LayerNorm(config.n_embd),                                    # final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)                 # \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        # Create position indices\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0).expand(b, t)\n",
    "        \n",
    "        # Create padding mask (True for real tokens, False for padding)\n",
    "        padding_mask = (idx != 512)\n",
    "        \n",
    "        # Get token and position embeddings\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        \n",
    "        # Sum token and position embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Apply transformer blocks with padding mask\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, padding_mask=padding_mask)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        # Get logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLLM(CLLMConfig)\n",
    "model.eval()\n",
    "#model.to('cuda')\n",
    "\n",
    "x = firstbatch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "#torch.cuda.manual_seed(42)\n",
    "\n",
    "while x.size(1) < 1024: #max_length:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
